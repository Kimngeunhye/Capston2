1. 로그인 기능 - 녹음 기능 -> 추후에 마이크 API로 백엔드가 전달
2. 회원가입 기능 - 데이터 입력 기능


First
1. negative/noise, negative/other, negative/silence 등에 등록된 사용자 1명 이외에 잡음, 타인의 목소리등을 녹음 한 후 wav로 저장
2. 긍정-사용자, 부정-잡음 -> 함께 사용해서 모델을 훈련시킨다.
X = np.array(positive_features + negative_features)
y = np.array([1]*len(positive_features) + [0]*len(negative_features))  # 1: 사용자, 0: 아님
3. 데이터와 레이블을 합쳐서 훈련 진행 후 끝난 후에는 모델 파일인 .h5에는 "음성 인식 모델의 가중치"와 "사용자가 등록된 음성과 부정적인 음성을 구분하는 방법만 저장"
4. .h5 -> 훈련된 모델의 가중치만 저장됨

Second
1. 여러사람의 음성이 들어올 경우 식별 정확도가 낮아짐
2. 모델이 사용자 개별 모델이랑 "본인/타인"만 분류 -> 무슨 말이냐, authenticate_user_with_rnn() 에서는 모든 사용자 모델을 하나씩 불러와서 "이게 너야?" 라고 묻는 방식 -> 그렇게 되면, 모델 간의 중복 응답 가능성이 발생
3. 사용자 간 음성 차이를 학습하지 못 한다. -> 각자의 모델은 자신이 아닌 다른 사람의 목소리에 대해 "이건 아니다" 라는 충분한 학습이 안 됨
-> 해결법
1. One-hot인코딩, y[]를 다중 분류로 처리한다. y = [0, 0, 0, 1, 1, 1, 2, 2, 2] 
2. 사용자 등록 시, 다른 사용자의 음성도 함께 학습에 사용하여 모델이 사용자 간의 차이를 학습한다.


MFCC와 RNN의 관계
MFCC는 "입력"을 만드는 전처리 과정이야.
RNN은 그 입력(MFCC 시퀀스)을 "분석하고 분류"하는 모델이야.

[음성 파일 (.wav)]
      ↓
[잡음 제거]
      ↓
[MFCC 추출] → (시간 순서에 따라 나열된 스펙트럼 특징값)
      ↓
[RNN 입력] → LSTM/GRU로 순차 정보 분석
      ↓
[출력] → Softmax로 사용자 분류 or Sigmoid로 이진 판별

즉,
🧠 한 줄 요약
👉 MFCC는 **RNN에 넣기 위한 숫자열(특징 시퀀스)**을 만들어주는 역할이고,
👉 RNN은 그 시퀀스를 보고 판단하는 역할이야.

MFCC는 ‘음성의 정체성’을 수치화해주는 거고, RNN은 그것이 누군지 맞히는 추론기계야.


Create_rnn()모델
모델의 흐름 설명
Conv1D: 처음에는 Conv1D를 사용하여 시계열 데이터를 처리하는데, 여기서 중요한 지역적 패턴을 추출합니다. 예를 들어, MFCC와 같은 특징 벡터에서 시간 축에 대한 패턴을 추출합니다.

MaxPooling1D: Conv1D의 출력은 풀링을 통해 차원을 축소합니다. 풀링을 통해 모델의 계산량을 줄이고, 중요한 특징만 남깁니다.

Bidirectional LSTM: LSTM을 양방향으로 처리하여 시퀀스 데이터를 양방향으로 학습합니다. return_sequences=True로 설정하여 첫 번째 LSTM은 시퀀스를 출력하게 하고, 두 번째 LSTM은 시퀀스의 마지막 상태만 출력하도록 설정합니다.

Dropout: 과적합을 방지하기 위해 LSTM 레이어와 Dense 레이어 뒤에 Dropout을 추가하여 학습 중 일부 뉴런을 무작위로 제외시킵니다.

Dense: 모델의 마지막 레이어에서 최종 클래스 예측을 위한 Dense 레이어가 추가됩니다. softmax 활성화 함수는 다중 클래스 분류를 위해 사용됩니다.